<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <meta name="robots" content="noindex">

    <title>Deniz Engin</title>
    <meta name="description" content="Deniz Engin | Personal Website">
    <meta name="author" content="Deniz Engin">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Deniz Engin">
    <meta name="twitter:description" content="Deniz Engin Twitter">
    <meta name="twitter:creator" content="Deniz Engin">


    <meta property="og:type" content="article">
    <meta property="og:title" content="Deniz Engin">
    <meta property="og:description" content="Deniz Engin | Personal Website">

    <meta name="msapplication-TileColor" content="#ffc40d">

    <meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" href="css/main.css?1589893399693513343">
    <link rel="canonical" href="http://localhost:4000">
    <link rel="alternate" type="application/rss+xml" title="Deniz Engin" href="feed.xml">
</head>


<body>


<header class="panel-cover panel-cover--collapsed left-bar" style="background-image: url(images/cover.jpg); ">
    <div class="panel-main">

        <div class="panel-main__inner panel-inverted">
            <div class="panel-main__content">
                <a href="" title="link to home of Deniz Engin"></a>
                <img src="images/profile.png" class="user-image" alt="My Profile Photo">
                <h1 class="panel-cover__title panel-title">Deniz Engin</h1>

                <hr class="panel-cover__divider panel-cover__divider--secondary">
                <a href="#about">
                    <p class="panel-cover__description">About</p>
                </a>
                <br>
                <a href="#news">
                    <p class="panel-cover__description">News</p>
                </a>
                <br>
                <a href="#research">
                    <p class="panel-cover__description">Research</p>
                </a>
                <br>
                <a href="#activies">
                    <p class="panel-cover__description">Activities</p>
                </a>

            </div>
        </div>

        <div class="panel-cover--overlay"></div>
    </div>
</header>


<div class="content-wrapper">
    <div class="content-wrapper__inner">

        <div id="about" class="main-post-list">

            <ol class="post-list">
                <li>
                    <h2 class="post-list__post-title post-title" style="font-size: 45px;">
                        Deniz Engin
                    </h2>
                    <p class="excerpt">
                    <p>
                        I'm a Computer Vision Engineer at <a
                            href="https://diffuse.ly/">Diffusely</a>, working on the <a
                            href="https://www.propershot.com/">ProperShot</a> product.
                        I completed my PhD with the <a href="https://www-linkmedia.irisa.fr/">LinkMedia</a> team
                        at <a
                            href="https://www.inria.fr/fr/centre-inria-rennes-bretagne-atlantique">INRIA Rennes</a>,
                        advised by <a
                            href="https://avrithis.net/">Yannis Avrithis</a>.
                        I received my BS and MS degrees in Electronics Engineering from <a
                            href="https://www.itu.edu.tr/ ">Istanbul Technical University.</a> My research
                        interests include vision and language applications, focusing on video understanding, video
                        question answering, and controllable image generation.
                    </p>
                    <p style="text-align:left">
                        <a href="mailto:denizengin255@gmail.com">Email</a> &nbsp/&nbsp
                        <a href="https://scholar.google.com/citations?user=5GQk5LUAAAAJ&hl">Google Scholar</a>
                        &nbsp/&nbsp
                        <a href="https://x.com/enginde_">Twitter</a> &nbsp/&nbsp
                        <a href="https://www.linkedin.com/in/denizenginn">LinkedIn</a> &nbsp/&nbsp
                        <a href="https://github.com/engindeniz">GitHub</a>
                    </p>


                    <hr class="post-list__divider">
                </li>

            </ol>

            <hr class="post-list__divider ">

        </div>


        <div id="news" class="main-post-list">

            <ol class="post-list">

                <li>
                    <h2 class="post-list__post-title post-title">
                        News
                    </h2>
                    <p class="excerpt">
                    <ul>
                        <li>Feb. 2024 - I joined <a
                                href="https://diffuse.ly/">Diffusely</a> as a Computer Vision Engineer.
                        </li>
                        <li>Jun. 2024 - I have successfully defended my <a href="thesis/DenizEngin_PhDThesis.pdf">PhD
                            thesis</a>!
                        </li>
                        <li>Oct. 2023 - <a href="https://github.com/engindeniz/vitis">ViTiS</a> code released.
                        </li>
                        <li>Sep. 2023 - Preprint released at arXiv: <a href="https://arxiv.org/abs/2309.15915">ViTiS</a>.
                        </li>
                        <li>Aug. 2023 - <a href="https://engindeniz.github.io/vitis">ViTiS</a> accepted at <a
                                href="https://iccv-clvl.github.io/2023/">ICCV 2023 CLVL Workshop</a> as an oral.
                        </li>
                        <li>Nov. 2022 - I visited IARAI for two weeks.
                        </li>
                        <li>Jul. 2022 - I participated in <a href="https://iplab.dmi.unict.it/icvss2022/">ICVSS 2022</a>.
                        </li>
                        <li>Jul. 2022 - I presented <a href="https://arxiv.org/abs/2103.14517">VideoQA</a> paper at <a
                                href="https://caprfiap2022.sciencesconf.org/">CAp & RFIAP 2022</a> as an oral.
                        </li>
                        <li>Oct. 2021 - I presented <a href="https://arxiv.org/abs/2103.14517">VideoQA</a> paper at <a
                                href="https://sites.google.com/view/iccv21clvl/"> ICCV 2021 CLVL Workshop</a>.
                        </li>

                        <li>Sep. 2021 - I participated in <a href="https://ellis.eu/events/ellis-doctoral-symposium">ELLIS
                            Doctoral Symposium 2021</a>.
                        </li>
                        <li>Aug. 2021 - <a href="https://github.com/InterDigitalInc/DialogSummary-VideoQA">Knowledge-based
                            VideoQA</a> code released.
                        </li>
                        <li>Jul. 2021 - <a href="https://engindeniz.github.io/dialogsummary-videoqa">Knowledge-based
                            VideoQA</a> accepted at <a href="https://iccv2021.thecvf.com/">ICCV 2021</a>.
                        </li>
                        <li>Jul. 2021 - I participated in <a href="https://project.inria.fr/paiss/paiss-2021/">PAISS
                            2021</a>.
                        </li>
                        <li>Mar. 2021 - Preprint released at arXiv: <a
                                href="https://arxiv.org/abs/2103.14517">Knowledge-based VideoQA</a>.
                        </li>
                        <li>Sep. 2020 - Starting my PhD at INRIA and InterDigital.</li>
                        <li>Apr. 2020 - Paper on <a
                                href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w48/Engin_Offline_Signature_Verification_on_Real-World_Documents_CVPRW_2020_paper.pdf">Signature
                            Verification</a> accepted at <a href="https://vislab.ucr.edu/Biometrics2020/index.php">CVPR
                            2020 Biometrics Workshop</a>.
                        </li>

                    </ul>
                    </p>

                    <hr class="post-list__divider">
                </li>

            </ol>

            <hr class="post-list__divider ">

        </div>


        <h2 id="research" class="post-list__post-title post-title">Research</h2>
        <div class="main-post-list">

            <ol class="post-list">
                <p class="excerpt">
                <p>See <a href="https://scholar.google.com.tr/citations?user=5GQk5LUAAAAJ&hl">Google Scholar</a>
                    for all publications.</p>

                <li>
                    <p class="excerpt">
                    <div class="two-col-row">
                        <div class="image-wrapper"><img src="images/thesis_cover.png"/></div>
                        <div class="right-col">
                            <strong>Video question answering with limited supervision</strong>
                            <br/>
                            <u>Deniz Engin</u>
                            <br/>
                            PhD Thesis 2024<br/>
                            <a class="abstract">abstract</a> |
                            <a href="thesis/DenizEngin_PhDThesis.pdf">pdf</a>

                        </div>
                    </div>
                    </p>

                    <div class="abstract">
                        <div style="width:50%">
                        </div>
                        <div class="right-col">
                            <p class="excerpt">
                            <p>Video content has significantly increased in volume and diversity in the digital era, and
                                this expansion has highlighted the necessity for advanced video understanding
                                technologies that transform vast volumes of unstructured data into practical insights by
                                learning from data. Driven by this necessity, this thesis explores semantically
                                understanding videos, leveraging multiple perceptual modes similar to human cognitive
                                processes and efficient learning with limited supervision similar to human learning
                                capabilities. Multimodal semantic video understanding synthesizes visual, audio, and
                                textual data to analyze and interpret video content, facilitating comprehension of
                                underlying semantics and context. This thesis specifically focuses on video question
                                answering to understand videos as one of the main video understanding tasks. Our first
                                contribution addresses long-range video question answering, which involves answering
                                questions about long videos, such as TV show episodes. These questions require an
                                understanding of extended video content. While recent approaches rely on human-generated
                                external sources, we present processing raw data to generate video summaries. Our
                                following contribution explores zero-shot and few-shot video question answering, aiming
                                to enhance efficient learning from limited data. We leverage the knowledge of existing
                                large-scale models by eliminating challenges in adapting pre-trained models to limited
                                data, such as overfitting, catastrophic forgetting, and bridging the cross-modal gap
                                between vision and language. We introduce a parameter-efficient method that combines
                                multimodal prompt learning with a transformer-based mapping network while keeping the
                                pre-trained vision and language models frozen. We demonstrate that these contributions
                                significantly enhance the capabilities of multimodal video question-answering systems,
                                where specifically human-annotated labeled data is limited or unavailable.</p>
                            </p>
                        </div>
                    </div>


                </li>

                <li>
                    <p class="excerpt">
                    <div class="two-col-row">
                        <div class="image-wrapper"><img src="images/vitis_method_overview.png"/></div>


                        <div class="right-col">
                            <strong>Zero-Shot and Few-Shot Video Question Answering with Multi-Modal Prompts</strong>
                            <br/>
                            <u>Deniz Engin</u>,
                            Yannis Avrithis
                            <br/>
                            ICCV Workshops 2023 <strong>[Oral Presentation]</strong><br/>
                            <a class="abstract">abstract</a> |
                            <a href="https://arxiv.org/abs/2309.15915">pdf</a> |
                            <a href="https://engindeniz.github.io/vitis">project page</a> |
                            <a href="https://github.com/engindeniz/vitis">code</a>

                        </div>
                    </div>
                    </p>

                    <div class="abstract">
                        <div style="width:50%">
                        </div>
                        <div class="right-col">
                            <p class="excerpt">
                            <p>Recent vision-language models are driven by large-scale pretrained
                                models. However, adapting pretrained models on limited data presents challenges such as
                                overfitting, catastrophic forgetting, and the cross-modal gap between vision and
                                language. We
                                introduce a parameter-efficient method to address these challenges, combining multimodal
                                prompt
                                learning and a transformer-based mapping network, while keeping the pretrained models
                                frozen.
                                Our experiments on several video question answering benchmarks demonstrate the
                                superiority of
                                our approach in terms of performance and parameter efficiency on
                                both zero-shot and few-shot settings.</p>
                            </p>
                        </div>
                    </div>


                </li>

                <li>
                    <p class="excerpt">
                    <div class="two-col-row">
                        <div class="image-wrapper"><img src="images/dialogsummary_videoqa.png"/></div>
                        <div class="right-col">
                            <strong>On the hidden treasure of dialog in video
                                question answering
                            </strong>
                            <br/>
                            <u>Deniz Engin</u>, François Schnitzler, Ngoc Q. K. Duong, Yannis Avrithis
                            <br/>
                            ICCV 2021<br/>
                            <a
                                    class="abstract">abstract</a> |
                            <a href="https://arxiv.org/abs/2103.14517">pdf</a> |
                            <a href="https://engindeniz.github.io/dialogsummary-videoqa">project page</a> |
                            <a href="https://github.com/InterDigitalInc/DialogSummary-VideoQA">code</a>
                        </div>
                    </div>
                    </p>

                    <div class="abstract">
                        <div style="width:50%">
                        </div>
                        <div class="right-col">
                            <p class="excerpt">
                            <p>High-level understanding of stories in video such as movies and TV shows from raw data is
                                extremely challenging. Modern video question answering (VideoQA) systems often use
                                additional human-made sources like plot synopses, scripts, video descriptions or
                                knowledge bases. In this work, we present a new approach to understand the whole story
                                without such external sources. The secret lies in the dialog: unlike any prior work, we
                                treat dialog as a noisy source to be converted into text description via dialog
                                summarization, much like recent methods treat video. The input of each modality is
                                encoded by transformers independently, and a simple fusion method combines all
                                modalities, using soft temporal attention for localization over long inputs. Our model
                                outperforms the state of the art on the KnowIT VQA dataset by a large margin, without
                                using question-specific human annotation or human-made plot summaries. It even
                                outperforms human evaluators who have never watched any whole episode before. Code is
                                available at https://engindeniz.github.io/dialogsummary-videoqa</p>
                            </p>
                        </div>
                    </div>

                </li>

                <li>


                    <div class="two-col-row">
                        <div class="image-wrapper">
                            <img src="images/signatureVerification.png"/>
                        </div>
                        <div class="right-col">
                            <strong>Offline Signature Verification on Real-World Documents</strong>
                            <br/><u>Deniz Engin*</u>, Alperen Kantarcı*, Seçil Arslan, Hazım Kemal Ekenel<br/>
                            CVPR Workshops 2020<br/>
                            <a class="abstract">abstract</a> |
                            <a href="https://arxiv.org/abs/2004.12104">pdf</a>
                        </div>
                    </div>
                    </p>

                    <div class="abstract">
                        <div style="width:50%">
                        </div>
                        <div class="right-col">
                            <p class="excerpt">
                            <p>Research on offline signature verification has explored a large variety of methods on
                                multiple signature datasets, which are collected under controlled conditions. However,
                                these datasets may not fully reflect the characteristics of the signatures in some
                                practical use cases. Real-world signatures extracted from the formal documents may
                                contain different types of occlusions, for example, stamps, company seals, ruling lines,
                                and signature boxes. Moreover, they may have very high intra-class variations, where
                                even genuine signatures resemble forgeries. In this paper, we address a real-world
                                writer independent offline signature verification problem, in which, a bank’s customers’
                                transaction request documents that contain their occluded signatures are compared with
                                their clean reference signatures. Our proposed method consists of two main components, a
                                stamp cleaning method based on CycleGAN and signature representation based on CNNs. We
                                extensively evaluate different verification setups, fine-tuning strategies, and
                                signature representation approaches to have a thorough analysis of the problem.
                                Moreover, we conduct a human evaluation to show the challenging nature of the problem.
                                We run experiments both on our custom dataset, as well as on the publicly available
                                Tobacco-800 dataset. The experimental results validate the difficulty of offline
                                signature verification on real-world documents. However, by employing the stamp cleaning
                                process, we improve the signature verification performance significantly.</p>
                            </p>
                        </div>
                    </div>

                </li>

                <li>
                    <p class="excerpt">
                    <div class="two-col-row">
                        <div class="image-wrapper"><img src="images/cycle.png"/></div>
                        <div class="right-col">
                            <strong>Cycle-Dehaze: Enhanced CycleGAN for Single Image Dehazing</strong>
                            <br/>
                            <strong>Deniz Engin*</strong>, Anıl Genç*, Hazım Kemal Ekenel
                            <br/>
                            CVPR Workshops 2018<br/>
                            <a class="abstract">abstract</a> |
                            <a href="https://arxiv.org/abs/1805.05308">pdf</a> |
                            <a href="https://github.com/engindeniz/Cycle-Dehaze">code</a>
                        </div>
                    </div>
                    </p>

                    <div class="abstract">
                        <div style="width:50%">
                        </div>
                        <div class="right-col">
                            <p class="excerpt">
                            <p>In this paper, we present an end-to-end network, called Cycle-Dehaze, for single image
                                dehazing problem, which does not require pairs of hazy and corresponding ground truth
                                images for training. That is, we train the network by feeding clean and hazy images in
                                an unpaired manner. Moreover, the proposed approach does not rely on estimation of the
                                atmospheric scattering model parameters. Our method enhances CycleGAN formulation by
                                combining cycle-consistency and perceptual losses in order to improve the quality of
                                textural information recovery and generate visually better haze-free images. Typically,
                                deep learning models for dehazing take low resolution images as input and produce low
                                resolution outputs. However, in the NTIRE 2018 challenge on single image dehazing, high
                                resolution images were provided. Therefore, we apply bicubic downscaling. After
                                obtaining low-resolution outputs from the network, we utilize the Laplacian pyramid to
                                upscale the output images to the original resolution. We conduct experiments on
                                NYU-Depth, I-HAZE, and O-HAZE datasets. Extensive experiments demonstrate that the
                                proposed approach improves CycleGAN method both quantitatively and qualitatively.</p>
                            </p>
                        </div>
                    </div>

                </li>

                <li>
                    <p class="excerpt">
                    <div class="two-col-row">
                        <div class="image-wrapper">
                            <img src="images/posenormalization.png"/>
                        </div>
                        <div class="right-col">
                            <strong>Face Frontalization for Cross-Pose Facial Expression Recognition</strong>
                            <br/>
                            <strong>Deniz Engin</strong>, Christophe Ecabert, Hazım Kemal Ekenel, Jean-Philippe Thiran
                            <br/>
                            EUSIPCO 2018<br/>
                            <a class="abstract">abstract</a> |
                            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8553087">pdf</a>
                        </div>
                    </div>
                    </p>

                    <div class="abstract">
                        <div style="width:50%">
                        </div>
                        <div class="right-col">
                            <p class="excerpt">
                            <p>In this paper, we have explored the effect of pose normalization for cross-pose facial
                                expression recognition. We have first presented an expression preserving face
                                frontalization method. After face frontalization step, for facial expression
                                representation and classification, we have employed both a traditional approach, by
                                using hand-crafted features, namely local binary patterns, in combination with support
                                vector machine classification and a relatively more recent approach based on
                                convolutional neural networks. To evaluate the impact of face frontalization on facial
                                expression recognition performance, we have conducted cross-pose, subject-independent
                                expression recognition experiments using the BU3DFE database. Experimental results show
                                that pose normalization improves the performance for cross-pose facial expression
                                recognition. Especially, when local binary patterns in combination with support vector
                                machine classifier is used, since this facial expression representation and
                                classification does not handle pose variations, the obtained performance increase is
                                significant. Convolutional neural networks-based approach is found to be more successful
                                handling pose variations, when it is fine-tuned on a dataset that contains face images
                                with varying pose angles. Its performance is further enhanced by benefiting from face
                                frontalization.</p>
                            </p>
                        </div>
                    </div>

                </li>

            </ol>


        </div>

        <hr class="post-list__divider">

        <div id="activies" class="main-post-list">

            <ol class="post-list">

                <li>
                    <h2 class="post-list__post-title post-title">
                        Activities
                    </h2>
                    <p class="excerpt">
                    <ul>
                        <li>[2023] - Reviewer at <a href="https://cvpr2023.thecvf.com/">CVPR</a>, <a
                                href="https://iccv2023.thecvf.com/">ICCV</a>.
                        </li>
                        <li>[2022] - Reviewer at <a href="https://eccv2022.ecva.net/">ECCV</a>.
                        </li>
                        <li>[2021] - Reviewer at <a
                                href="https://2021.ieeeicme.org/2021.ieeeicme.org/index.html">ICME</a>.
                        </li>


                    </ul>
                    </p>

                </li>

            </ol>


        </div>


    </div>

    <script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script type="text/javascript" src="js/main.js?1589893399693513343"></script>


</div>
</body>
</html>